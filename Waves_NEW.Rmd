---
title: "Waves_NEW"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(include = FALSE)

# check dependencies
packages = c("readxl",
             "data.table",
             "forecast",
             "ggplot2",
             "xts")
lapply(packages, require, character.only = T)

# load sources
source("paths.R")
source("getData.R")
source("tidyData.R")

```

```{r read_data}
if (!file.exists("raw.Rda")){
# link to a 0.6GB file including all data, no texts, in CSV format
        DT <- fread(input = paste(NEW_DATA_PATH, "all_files_no_text.csv", sep = "/"), sep = ",", quote = '"')
} else {
        load("raw.Rda")
}
```

```{r context}
# filter only the context(s) of interest
setkey(DT, "context")
DT <- DT["American Politics",]
```

```{r site}
# read only US websites names
WEBSITES = readWebsites()
# filter DT by WEBSITES
DT <- DT[site %in% WEBSITES$site,]
```

```{r state}
# read states list
STATES = readStates()
# read association of colors to states
COLORS = readColors()
STATES = merge(STATES, COLORS, all = T, by = "state")
# add states (of origin of sources):
setkey(DT,site)
setkey(WEBSITES, site)
DT = merge(DT,WEBSITES, all = F, all.x = T)
```

```{r dates}
# split PUBLISH_DATE to publish_date and publish_time
DT[,publish_time := sapply(strsplit(publish_date,' '), "[", 2)][,publish_date := sapply(strsplit(publish_date,' '), "[", 1)]
DT[, publish_date := as.Date(publish_date)]

# get full date range (in days)
DATES <-  getDates(DT)
```

```{r topics}
# extract list of topics in dataset
topicCols <- colnames(DT)[c(11:19, 21:230)]
# select topics with more than 30 data points
relevant <- sapply(seq_along(topicCols), function(i) {length(unique(DT[, get(topicCols[i])]))}) > 30
# set TOPICS for relevant topics only
TOPICS <- topicCols[relevant]

byTopic <- lapply(seq_along(TOPICS), function(i){
        DT[!is.na(get(TOPICS[i])), c("publish_date", "state", "site", "title", TOPICS[i]), with = FALSE]
})
```

```{r counts}

# how many articles per day per site? (returns list of topics)
articleCountPerSite <- lapply(seq_along(TOPICS), function(i){
        getAggregated(byTopic[[i]], grpby = "site")
})
# how many articles per day per state? (returns list of topics)
articleCountPerState <- lapply(seq_along(TOPICS), function(i){
        getAggregated(byTopic[[i]], grpby = "state")
})

# check if there are no rows without topic in DT (sanity)
if (length(DT[, rowSums(.SD, na.rm = T), .SDcols = TOPICS]) == nrow(DT)){
        TotalArticleCountPerSite <- getAggregated(DT, grpby = "site")
        TotalArticleCountPerState <- getAggregated(DT, grpby = "state")
}

# calculate daily Salience per site
salience.site <- lapply(seq_along(TOPICS), function(i){
        tmp <- merge(x = articleCountPerSite[[i]], 
                     y = TotalArticleCountPerSite,
                     by = c("publish_date", "site"),
                     all.x = T)
        tmp[, SaliencePerSite := N.x/N.y]
        return(tmp)
})

# calculate daily Salience per state
salience.state <- lapply(seq_along(TOPICS), function(i){
        tmp <- merge(x = articleCountPerState[[i]], 
                     y = TotalArticleCountPerState,
                     by = c("publish_date", "state"),
                     all.x = T)
        tmp[, SaliencePerState := N.x/N.y]
        return(tmp)
})
```

```{r scores}

# add states (of origin of sources):
setkey(WEBSITES, site)

salience <- lapply(seq_along(TOPICS), function(i){
        setkey(salience.site[[i]], site)
        tmp <- merge(salience.site[[i]], WEBSITES, all = F, all.x = T)
        merge(x = tmp[, .(publish_date, SaliencePerSite, site, state)], y = salience.state[[i]][, .(publish_date, SaliencePerState, state)], by = c("publish_date", "state"))
})

setkey(STATES, color)
# calculate daily averages
lapply(seq_along(TOPICS), function(i){
# salience per site
        salience[[i]][, DailySaliencePerSite := lapply(.SD, mean), by = c("publish_date"), .SDcols = "SaliencePerSite"]
# salience per state
        salience[[i]][, DailySaliencePerState := lapply(.SD, mean), by = c("publish_date"), .SDcols = "SaliencePerState"]
# group sources by states:
        setkey(salience[[i]], "state")
# salience in NATIONWIDE group
        salience[[i]]["Nationwide", DailyNationwideSalience := lapply(.SD, mean), by = c("publish_date"), .SDcols = "SaliencePerSite"]
# salience in LOCAL group
        salience[[i]][!"Nationwide", DailyLocalSalience := lapply(.SD, mean), by = c("publish_date"), .SDcols = "SaliencePerSite"]
# salience in BLUE group
        salience[[i]][STATES["Blue", state], DailyBlueSalience := lapply(.SD, mean), by = c("publish_date"), .SDcols = "SaliencePerSite"]
# salience in RED group
        salience[[i]][STATES["Red", state], DailyRedSalience := lapply(.SD, mean), by = c("publish_date"), .SDcols = "SaliencePerSite"]
# salience in PURPLE group
        salience[[i]][STATES["Purple", state], DailyPurpleSalience := lapply(.SD, mean), by = c("publish_date"), .SDcols = "SaliencePerSite"]
})

byTopic <- lapply(seq_along(TOPICS), function(i){
        merge(x = byTopic[[i]], y = salience[[i]], by = c("publish_date", "site", "state"), all = T)
})

```

```{r padding}
# do padding of dates set missing dates values to 0 (instead fo NA)
byTopic <- lapply(seq_along(TOPICS), function(i){
        setDates(byTopic[[i]], DATES)
})
```

```{r descriptive}
scores <- colnames(byTopic[[1]])[8:14]
byTopic[[1]][, sapply(.SD, summary), .SDcols = scores]
```

```{r group}

byTopic <- lapply(seq_along(TOPICS), function(i){
        melt(data = byTopic[[i]], measure.vars = scores)
})
# byTopics[[1]][, value := na.fill(value,0)]
```



```{r plot}
qplot(data = byTopics[[1]], x = publish_date, y = value) + geom_line() + facet_wrap(~sourceGroup, ncol = 2)

# add shiny app/ slidify
```
